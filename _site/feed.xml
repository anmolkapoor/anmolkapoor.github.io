<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-03-22T23:02:57+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Anmol Kapoor</title><subtitle>~Working on tag line~</subtitle><author><name>Anmol Kapoor</name></author><entry><title type="html">Arxiv-topics - Explore &amp;amp; Discover Research Papers on Arxiv repository using Topics Learned from Data</title><link href="http://localhost:4000/2019/03/19/arxiv-topics/" rel="alternate" type="text/html" title="Arxiv-topics - Explore &amp; Discover Research Papers on Arxiv repository using Topics Learned from Data" /><published>2019-03-19T00:00:00+05:30</published><updated>2019-03-19T00:00:00+05:30</updated><id>http://localhost:4000/2019/03/19/arxiv-topics</id><content type="html" xml:base="http://localhost:4000/2019/03/19/arxiv-topics/">Arxiv repository [https://arxiv.org/] hosts 1.5m academic papers, and users add over 10K papers per month. Keeping track of trends is challenging as it is limited to keyword search, and does not let users track and explore content based on themes that capture relationships beyond shared words, or recommend content from this perspective. This challenged us to use unsupervised learning to learn topics to summarise documents, and let users explore and find research papers on similar topics.

Our approach is divided in three steps:

First, For the topic discovery we used LDA topic model. In simple terms, with given documents and their words, the purpose of LDA is to learn the representation of fixed number of topics from the corpus and their distribution. We also experimented with  with other topic models such as LSA, contrasted their result and found LDA topics to be more accurate and interpretable.

Second, At the core of our web application, we use pyLDAvis, to visulaize and let user explore individual topics and its associated terms.
Whats new in our approach is third step which now allow user to find research paper similar to its topic mix. For example, a user reading research paper on theme of Computer vision and Human Computer Interaction. can easily find another paper around same theme.

For experiments we  download 26,000 research papers through arxiv api[https://arxiv.org/help/api/index], lemmatised them, applied stemming, and examined top 1000 keywords manually

To evaluate a topic model, we calculated Topic coherence which is measures of topic consistency. Since it is unsupervised learning for our experiments we generated visualisations of over a topic range from 3 to 100. Experiment resulted in topics which are diverse yet well defined and can be given well defined names like Language, LifeScience, Theory etc.

## Quick walkthrough video with demo
###### (3 min watch)
-----
[![You tube screenshot](https://raw.githubusercontent.com/anmolkapoor/project-explore-arxiv-with-topic-modelling/master/documents/youtube-video-screenshot.png)](https://www.youtube.com/watch?v=grQj8xCZtdo &quot;Quick walkthrough video with demo  - Click to Watch!&quot;)

## Running app demo (on Heroku)
###### (Please allow 30 sec to wake up sleeping(zzz...) dyno
-----
[![Demo screenshot](https://raw.githubusercontent.com/anmolkapoor/project-explore-arxiv-with-topic-modelling/master/documents/demo-screenshot.png)](https://boiling-thicket-31500.herokuapp.com/ &quot;Demo on Heroku  - Click to view!&quot;)

## Detailed project details
###### 15 min read
-----
[Detailed  Report PDF ](https://github.com/anmolkapoor/project-explore-arxiv-with-topic-modelling/blob/master/documents/report.pdf)</content><author><name>Anmol Kapoor</name></author><summary type="html">Arxiv repository [https://arxiv.org/] hosts 1.5m academic papers, and users add over 10K papers per month. Keeping track of trends is challenging as it is limited to keyword search, and does not let users track and explore content based on themes that capture relationships beyond shared words, or recommend content from this perspective. This challenged us to use unsupervised learning to learn topics to summarise documents, and let users explore and find research papers on similar topics.</summary></entry><entry><title type="html">Kullback-Leibler Divergence Explained</title><link href="http://localhost:4000/2019/03/16/kullback-leibler-divergence-explained/" rel="alternate" type="text/html" title="Kullback-Leibler Divergence Explained" /><published>2019-03-16T00:00:00+05:30</published><updated>2019-03-16T00:00:00+05:30</updated><id>http://localhost:4000/2019/03/16/kullback-leibler-divergence-explained</id><content type="html" xml:base="http://localhost:4000/2019/03/16/kullback-leibler-divergence-explained/">This post is about comparing two probablistic distributions and the measure of loss of infromation that will happen if one is represented by other. In real world, proabibilty distributions depicts the probability of different variables that can be taken by a variable. For this example, we will consider probability distribution of decrete values of a variable. To relate to a situation lets go through an example.

Suppose in galaxy far far away, R2D2 was given a task by princess &lt;#todo&gt; to store th plans for the death star. These plans are the possible positions of the target quardrant, firing on which can detroy death star. Pricess gives the task to R2D2 to transmit this information to the rebels. 

The data looks like this:

The above data is about 6 mutually exculsive places and their success count (each instanc)

In its escape, R2D2 damages its superior communication ability and moved down to transmit only 2 variables of information. Luckily it these two variables are enough to transmit two other well known proabability distributions : Descrete Uniform distribution and Binomial distribution

Discrete Uniform distrubtion, which is well understood by the rebels show</content><author><name>Anmol Kapoor</name></author><summary type="html">This post is about comparing two probablistic distributions and the measure of loss of infromation that will happen if one is represented by other. In real world, proabibilty distributions depicts the probability of different variables that can be taken by a variable. For this example, we will consider probability distribution of decrete values of a variable. To relate to a situation lets go through an example.</summary></entry></feed>